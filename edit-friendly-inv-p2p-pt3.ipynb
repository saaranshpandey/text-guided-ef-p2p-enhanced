{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10004470,"sourceType":"datasetVersion","datasetId":6158266},{"sourceId":10045192,"sourceType":"datasetVersion","datasetId":6188435}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:25:06.957969Z","iopub.execute_input":"2024-11-29T16:25:06.958330Z","iopub.status.idle":"2024-11-29T16:25:08.970207Z","shell.execute_reply.started":"2024-11-29T16:25:06.958299Z","shell.execute_reply":"2024-11-29T16:25:08.969359Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"--2024-11-29 16:25:07--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nResolving repo.anaconda.com (repo.anaconda.com)... 104.16.32.241, 104.16.191.158, 2606:4700::6810:bf9e, ...\nConnecting to repo.anaconda.com (repo.anaconda.com)|104.16.32.241|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 148337011 (141M) [application/octet-stream]\nSaving to: 'miniconda.sh'\n\nminiconda.sh        100%[===================>] 141.46M   167MB/s    in 0.8s    \n\n2024-11-29 16:25:08 (167 MB/s) - 'miniconda.sh' saved [148337011/148337011]\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!bash miniconda.sh -b -p /kaggle/working/miniconda","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:25:08.972171Z","iopub.execute_input":"2024-11-29T16:25:08.972539Z","iopub.status.idle":"2024-11-29T16:25:20.004233Z","shell.execute_reply.started":"2024-11-29T16:25:08.972499Z","shell.execute_reply":"2024-11-29T16:25:20.003032Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"PREFIX=/kaggle/working/miniconda\nUnpacking payload ...\n\nInstalling base environment...\n\nPreparing transaction: ...working... done\nExecuting transaction: ...working... done\ninstallation finished.\nWARNING:\n    You currently have a PYTHONPATH environment variable set. This may cause\n    unexpected behavior when running the Python interpreter in Miniconda3.\n    For best results, please verify that your PYTHONPATH only points to\n    directories of packages that are compatible with the Python interpreter\n    in Miniconda3: /kaggle/working/miniconda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"PATH\"] = \"/kaggle/working/miniconda/bin:\" + os.environ[\"PATH\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:25:20.006189Z","iopub.execute_input":"2024-11-29T16:25:20.006479Z","iopub.status.idle":"2024-11-29T16:25:20.011273Z","shell.execute_reply.started":"2024-11-29T16:25:20.006451Z","shell.execute_reply":"2024-11-29T16:25:20.010290Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!conda --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:25:20.012663Z","iopub.execute_input":"2024-11-29T16:25:20.013072Z","iopub.status.idle":"2024-11-29T16:25:21.395215Z","shell.execute_reply.started":"2024-11-29T16:25:20.013034Z","shell.execute_reply":"2024-11-29T16:25:21.394323Z"}},"outputs":[{"name":"stdout","text":"conda 24.9.2\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!conda create -n env python=3.9 -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:25:21.397703Z","iopub.execute_input":"2024-11-29T16:25:21.398009Z","iopub.status.idle":"2024-11-29T16:25:54.189796Z","shell.execute_reply.started":"2024-11-29T16:25:21.397981Z","shell.execute_reply":"2024-11-29T16:25:54.188444Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /kaggle/working/miniconda/envs/env\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n    bzip2-1.0.8                |       h4bc722e_7         247 KB  conda-forge\n    ca-certificates-2024.8.30  |       hbcca054_0         155 KB  conda-forge\n    ld_impl_linux-64-2.43      |       h712a8e2_2         654 KB  conda-forge\n    libffi-3.4.2               |       h7f98852_5          57 KB  conda-forge\n    libgcc-14.2.0              |       h77fa898_1         829 KB  conda-forge\n    libgcc-ng-14.2.0           |       h69a702a_1          53 KB  conda-forge\n    libgomp-14.2.0             |       h77fa898_1         450 KB  conda-forge\n    libnsl-2.0.1               |       hd590300_0          33 KB  conda-forge\n    libsqlite-3.47.0           |       hadc24fc_1         855 KB  conda-forge\n    libuuid-2.38.1             |       h0b41bf4_0          33 KB  conda-forge\n    libxcrypt-4.4.36           |       hd590300_1          98 KB  conda-forge\n    libzlib-1.3.1              |       hb9d3cd8_2          60 KB  conda-forge\n    ncurses-6.5                |       he02047a_1         868 KB  conda-forge\n    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n    pip-24.3.1                 |     pyh8b19718_0         1.2 MB  conda-forge\n    python-3.9.20              |h13acc7a_1_cpython        22.6 MB  conda-forge\n    readline-8.2               |       h8228510_1         275 KB  conda-forge\n    setuptools-75.6.0          |     pyhff2d567_1         756 KB  conda-forge\n    tk-8.6.13                  |noxft_h4845f30_101         3.2 MB  conda-forge\n    tzdata-2024b               |       hc8b5060_0         119 KB  conda-forge\n    wheel-0.45.1               |     pyhd8ed1ab_0          62 KB  conda-forge\n    xz-5.2.6                   |       h166bdaf_0         409 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        35.6 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n  bzip2              conda-forge/linux-64::bzip2-1.0.8-h4bc722e_7 \n  ca-certificates    conda-forge/linux-64::ca-certificates-2024.8.30-hbcca054_0 \n  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.43-h712a8e2_2 \n  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n  libgcc-ng          conda-forge/linux-64::libgcc-ng-14.2.0-h69a702a_1 \n  libgomp            conda-forge/linux-64::libgomp-14.2.0-h77fa898_1 \n  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n  libsqlite          conda-forge/linux-64::libsqlite-3.47.0-hadc24fc_1 \n  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n  ncurses            conda-forge/linux-64::ncurses-6.5-he02047a_1 \n  openssl            conda-forge/linux-64::openssl-3.4.0-hb9d3cd8_0 \n  pip                conda-forge/noarch::pip-24.3.1-pyh8b19718_0 \n  python             conda-forge/linux-64::python-3.9.20-h13acc7a_1_cpython \n  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n  setuptools         conda-forge/noarch::setuptools-75.6.0-pyhff2d567_1 \n  tk                 conda-forge/linux-64::tk-8.6.13-noxft_h4845f30_101 \n  tzdata             conda-forge/noarch::tzdata-2024b-hc8b5060_0 \n  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_0 \n  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n\n\n\nDownloading and Extracting Packages:\npython-3.9.20        | 22.6 MB   |                                       |   0% \ntk-8.6.13            | 3.2 MB    |                                       |   0% \u001b[A\n\nopenssl-3.4.0        | 2.8 MB    |                                       |   0% \u001b[A\u001b[A\n\n\npip-24.3.1           | 1.2 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nncurses-6.5          | 868 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nlibsqlite-3.47.0     | 855 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nlibgcc-14.2.0        | 829 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nsetuptools-75.6.0    | 756 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nld_impl_linux-64-2.4 | 654 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibgomp-14.2.0       | 450 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nxz-5.2.6             | 409 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nreadline-8.2         | 275 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nbzip2-1.0.8          | 247 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nca-certificates-2024 | 155 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\ntzdata-2024b         | 119 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibxcrypt-4.4.36     | 98 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwheel-0.45.1         | 62 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibzlib-1.3.1        | 60 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibffi-3.4.2         | 57 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibgcc-ng-14.2.0     | 53 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibuuid-2.38.1       | 33 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibnsl-2.0.1         | 33 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_openmp_mutex-4.5    | 23 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython-3.9.20        | 22.6 MB   | 3                                     |   1% [A\u001b[A\u001b[A\u001b[A\n\nopenssl-3.4.0        | 2.8 MB    | #8                                    |   5% \u001b[A\u001b[A\n\n\npip-24.3.1           | 1.2 MB    | ####3                                 |  12% \u001b[A\u001b[A\u001b[A\ntk-8.6.13            | 3.2 MB    | ##1                                   |   6% \u001b[A\n\n\n\npython-3.9.20        | 22.6 MB   | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nlibsqlite-3.47.0     | 855 KB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\npython-3.9.20        | 22.6 MB   | ###########7                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nsetuptools-75.6.0    | 756 KB    | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibgomp-14.2.0       | 450 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nld_impl_linux-64-2.4 | 654 KB    | 9                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npython-3.9.20        | 22.6 MB   | ################4                     |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nreadline-8.2         | 275 KB    | ##1                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nbzip2-1.0.8          | 247 KB    | ##3                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nca-certificates-2024 | 155 KB    | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npython-3.9.20        | 22.6 MB   | #####################1                |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibxcrypt-4.4.36     | 98 KB     | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwheel-0.45.1         | 62 KB     | #########6                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibzlib-1.3.1        | 60 KB     | #########9                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibffi-3.4.2         | 57 KB     | ##########3                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython-3.9.20        | 22.6 MB   | ##########################7           |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibuuid-2.38.1       | 33 KB     | ##################                    |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibnsl-2.0.1         | 33 KB     | ##################1                   |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_openmp_mutex-4.5    | 23 KB     | #########################6            |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython-3.9.20        | 22.6 MB   | #################################9    |  92% [A\u001b[A\u001b[A\u001b[A\n\nopenssl-3.4.0        | 2.8 MB    | ##################################### | 100% \u001b[A\u001b[A\n\nopenssl-3.4.0        | 2.8 MB    | ##################################### | 100% \u001b[A\u001b[A\n\n\npip-24.3.1           | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\npip-24.3.1           | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\n\nlibsqlite-3.47.0     | 855 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nlibsqlite-3.47.0     | 855 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nlibgcc-14.2.0        | 829 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nlibgcc-14.2.0        | 829 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibgomp-14.2.0       | 450 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibgomp-14.2.0       | 450 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ntk-8.6.13            | 3.2 MB    | ##################################### | 100% \u001b[A\ntk-8.6.13            | 3.2 MB    | ##################################### | 100% \u001b[A\n\n\n\n\n\n\n\nld_impl_linux-64-2.4 | 654 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nld_impl_linux-64-2.4 | 654 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nsetuptools-75.6.0    | 756 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nsetuptools-75.6.0    | 756 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nxz-5.2.6             | 409 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nxz-5.2.6             | 409 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nreadline-8.2         | 275 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nreadline-8.2         | 275 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nca-certificates-2024 | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nca-certificates-2024 | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nbzip2-1.0.8          | 247 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nbzip2-1.0.8          | 247 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibxcrypt-4.4.36     | 98 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibxcrypt-4.4.36     | 98 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwheel-0.45.1         | 62 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwheel-0.45.1         | 62 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibzlib-1.3.1        | 60 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibzlib-1.3.1        | 60 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibffi-3.4.2         | 57 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibffi-3.4.2         | 57 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibgcc-ng-14.2.0     | 53 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibgcc-ng-14.2.0     | 53 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibuuid-2.38.1       | 33 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibuuid-2.38.1       | 33 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibnsl-2.0.1         | 33 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibnsl-2.0.1         | 33 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\ntzdata-2024b         | 119 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\ntzdata-2024b         | 119 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nncurses-6.5          | 868 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\npython-3.9.20        | 22.6 MB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate env\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# !source activate env && conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:42:27.298111Z","iopub.execute_input":"2024-11-29T02:42:27.298424Z","iopub.status.idle":"2024-11-29T02:42:27.302590Z","shell.execute_reply.started":"2024-11-29T02:42:27.298393Z","shell.execute_reply":"2024-11-29T02:42:27.301799Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# # Install Git if not already available\n!apt-get install git\n\n# # Clone the PnPInversion repository\n!git clone https://github.com/cure-lab/PnPInversion.git\n%cd PnPInversion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:25:54.191844Z","iopub.execute_input":"2024-11-29T16:25:54.192250Z","iopub.status.idle":"2024-11-29T16:26:01.179311Z","shell.execute_reply.started":"2024-11-29T16:25:54.192212Z","shell.execute_reply":"2024-11-29T16:26:01.178375Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit is already the newest version (1:2.34.1-1ubuntu1.11).\n0 upgraded, 0 newly installed, 0 to remove and 68 not upgraded.\nCloning into 'PnPInversion'...\nremote: Enumerating objects: 506, done.\u001b[K\nremote: Counting objects: 100% (111/111), done.\u001b[K\nremote: Compressing objects: 100% (100/100), done.\u001b[K\nremote: Total 506 (delta 19), reused 96 (delta 11), pack-reused 395 (from 1)\u001b[K\nReceiving objects: 100% (506/506), 87.13 MiB | 52.42 MiB/s, done.\nResolving deltas: 100% (87/87), done.\n/kaggle/working/PnPInversion\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\noutput_dir = '/kaggle/working/PnPInversion/output/edit-friendly-inversion+p2p/annotation_images'\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"'output' directory created at: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:26:01.180755Z","iopub.execute_input":"2024-11-29T16:26:01.181078Z","iopub.status.idle":"2024-11-29T16:26:01.186614Z","shell.execute_reply.started":"2024-11-29T16:26:01.181048Z","shell.execute_reply":"2024-11-29T16:26:01.185729Z"}},"outputs":[{"name":"stdout","text":"'output' directory created at: /kaggle/working/PnPInversion/output/edit-friendly-inversion+p2p/annotation_images\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# !rm -rf /kaggle/working/PnPInversion/output/edit-friendly-inversion+p2p/annotation_images/output-edit-friendly-p2p/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:42:33.983164Z","iopub.execute_input":"2024-11-29T02:42:33.983502Z","iopub.status.idle":"2024-11-29T02:42:33.990422Z","shell.execute_reply.started":"2024-11-29T02:42:33.983466Z","shell.execute_reply":"2024-11-29T02:42:33.989686Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!cp -r /kaggle/input/output-edit-friendly-p2p/* /kaggle/working/PnPInversion/output/edit-friendly-inversion+p2p/annotation_images/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:26:01.188620Z","iopub.execute_input":"2024-11-29T16:26:01.188950Z","iopub.status.idle":"2024-11-29T16:26:08.839420Z","shell.execute_reply.started":"2024-11-29T16:26:01.188915Z","shell.execute_reply":"2024-11-29T16:26:08.838419Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!source activate env && pip install -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:26:08.840872Z","iopub.execute_input":"2024-11-29T16:26:08.841220Z","iopub.status.idle":"2024-11-29T16:28:27.911401Z","shell.execute_reply.started":"2024-11-29T16:26:08.841185Z","shell.execute_reply":"2024-11-29T16:28:27.910487Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting diffusers==0.10.0 (from -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading diffusers-0.10.0-py3-none-any.whl.metadata (31 kB)\nCollecting transformers (from -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 2))\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\nCollecting ftfy (from -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 3))\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting opencv-python (from -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 4))\n  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting ipywidgets (from -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\nCollecting matplotlib (from -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading matplotlib-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting accelerate (from -r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nCollecting importlib-metadata (from diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\nCollecting filelock (from diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting huggingface-hub>=0.10.0 (from diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\nCollecting numpy (from diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nCollecting regex!=2019.12.17 (from diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\nCollecting requests (from diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting Pillow (from diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\nCollecting packaging>=20.0 (from transformers->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 2))\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pyyaml>=5.1 (from transformers->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 2))\n  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\nCollecting tokenizers<0.21,>=0.20 (from transformers->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 2))\n  Downloading tokenizers-0.20.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting safetensors>=0.4.1 (from transformers->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 2))\n  Downloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting tqdm>=4.27 (from transformers->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 2))\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting wcwidth (from ftfy->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 3))\n  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\nCollecting comm>=0.1.3 (from ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\nCollecting ipython>=6.1.0 (from ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\nCollecting traitlets>=4.3.1 (from ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\nCollecting widgetsnbextension~=4.0.12 (from ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\nCollecting jupyterlab-widgets~=3.0.12 (from ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\nCollecting contourpy>=1.0.1 (from matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\nCollecting cycler>=0.10 (from matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools>=4.22.0 (from matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading fonttools-4.55.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\nCollecting kiwisolver>=1.3.1 (from matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\nCollecting pyparsing>=2.3.1 (from matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\nCollecting python-dateutil>=2.7 (from matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting importlib-resources>=3.2.0 (from matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\nCollecting psutil (from accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\nCollecting torch>=1.10.0 (from accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting fsspec>=2023.5.0 (from huggingface-hub>=0.10.0->diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.10.0->diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting decorator (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\nCollecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting matplotlib-inline (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\nCollecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\nCollecting pygments>=2.4.0 (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\nCollecting stack-data (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\nCollecting exceptiongroup (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\nCollecting pexpect>4.3 (from ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\nCollecting six>=1.5 (from python-dateutil>=2.7->matplotlib->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 6))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting networkx (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\nCollecting jinja2 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.1.0 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nCollecting sympy==1.13.1 (from torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\nCollecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nCollecting charset-normalizer<4,>=2 (from requests->diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading charset_normalizer-3.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nCollecting idna<4,>=2.5 (from requests->diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\nCollecting urllib3<3,>=1.21.1 (from requests->diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\nCollecting certifi>=2017.4.17 (from requests->diffusers==0.10.0->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 1))\n  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\nCollecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\nCollecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting MarkupSafe>=2.0 (from jinja2->torch>=1.10.0->accelerate->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 7))\n  Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting executing>=1.2.0 (from stack-data->ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\nCollecting asttokens>=2.1.0 (from stack-data->ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\nCollecting pure-eval (from stack-data->ipython>=6.1.0->ipywidgets->-r /kaggle/working/PnPInversion/environment/p2p_requirements.txt (line 5))\n  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nDownloading diffusers-0.10.0-py3-none-any.whl (502 kB)\nDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\nDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\nDownloading matplotlib-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\nDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\nDownloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\nDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nDownloading fonttools-4.55.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\nDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\nDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\nDownloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m153.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\nDownloading pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\nDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\nDownloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\nDownloading tokenizers-0.20.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m146.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\nDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\nDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\nDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\nDownloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\nDownloading requests-2.32.3-py3-none-any.whl (64 kB)\nDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\nDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\nDownloading charset_normalizer-3.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\nDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\nDownloading idna-3.10-py3-none-any.whl (70 kB)\nDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\nDownloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\nDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\nDownloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\nDownloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\nDownloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\nDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\nDownloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\nDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\nDownloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\nDownloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\nDownloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\nDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\nDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\nDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\nInstalling collected packages: wcwidth, pure-eval, ptyprocess, mpmath, zipp, widgetsnbextension, urllib3, typing-extensions, traitlets, tqdm, sympy, six, safetensors, regex, pyyaml, pyparsing, pygments, psutil, prompt-toolkit, Pillow, pexpect, parso, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, jupyterlab-widgets, idna, ftfy, fsspec, fonttools, filelock, executing, exceptiongroup, decorator, cycler, charset-normalizer, certifi, triton, requests, python-dateutil, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib-inline, jinja2, jedi, importlib-resources, importlib-metadata, contourpy, comm, asttokens, stack-data, nvidia-cusolver-cu12, matplotlib, huggingface-hub, torch, tokenizers, ipython, diffusers, transformers, ipywidgets, accelerate\nSuccessfully installed MarkupSafe-3.0.2 Pillow-11.0.0 accelerate-1.1.1 asttokens-2.4.1 certifi-2024.8.30 charset-normalizer-3.4.0 comm-0.2.2 contourpy-1.3.0 cycler-0.12.1 decorator-5.1.1 diffusers-0.10.0 exceptiongroup-1.2.2 executing-2.1.0 filelock-3.16.1 fonttools-4.55.0 fsspec-2024.10.0 ftfy-6.3.1 huggingface-hub-0.26.3 idna-3.10 importlib-metadata-8.5.0 importlib-resources-6.4.5 ipython-8.18.1 ipywidgets-8.1.5 jedi-0.19.2 jinja2-3.1.4 jupyterlab-widgets-3.0.13 kiwisolver-1.4.7 matplotlib-3.9.2 matplotlib-inline-0.1.7 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opencv-python-4.10.0.84 packaging-24.2 parso-0.8.4 pexpect-4.9.0 prompt-toolkit-3.0.48 psutil-6.1.0 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.18.0 pyparsing-3.2.0 python-dateutil-2.9.0.post0 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 six-1.16.0 stack-data-0.6.3 sympy-1.13.1 tokenizers-0.20.3 torch-2.5.1 tqdm-4.67.1 traitlets-5.14.3 transformers-4.46.3 triton-3.1.0 typing-extensions-4.12.2 urllib3-2.2.3 wcwidth-0.2.13 widgetsnbextension-4.0.13 zipp-3.21.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!source activate env && conda config --add channels defaults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:28:27.912978Z","iopub.execute_input":"2024-11-29T16:28:27.913361Z","iopub.status.idle":"2024-11-29T16:28:29.880347Z","shell.execute_reply.started":"2024-11-29T16:28:27.913321Z","shell.execute_reply":"2024-11-29T16:28:29.879358Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!source activate env && conda install intel-openmp -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:28:29.881675Z","iopub.execute_input":"2024-11-29T16:28:29.881972Z","iopub.status.idle":"2024-11-29T16:28:40.279159Z","shell.execute_reply.started":"2024-11-29T16:28:29.881933Z","shell.execute_reply":"2024-11-29T16:28:40.278296Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Channels:\n - defaults\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /kaggle/working/miniconda/envs/env\n\n  added / updated specs:\n    - intel-openmp\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2024.11.26 |       h06a4308_0         131 KB\n    intel-openmp-2023.1.0      |   hdb19cb5_46306        17.2 MB\n    libsqlite-3.46.0           |       hde9e2c9_0         845 KB  conda-forge\n    libzlib-1.2.13             |       h4ab18f5_6          60 KB  conda-forge\n    python-3.9.19              |h0755675_0_cpython        22.7 MB  conda-forge\n    zlib-1.2.13                |       h4ab18f5_6          91 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        41.0 MB\n\nThe following NEW packages will be INSTALLED:\n\n  intel-openmp       pkgs/main/linux-64::intel-openmp-2023.1.0-hdb19cb5_46306 \n  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n  zlib               conda-forge/linux-64::zlib-1.2.13-h4ab18f5_6 \n\nThe following packages will be UPDATED:\n\n  ca-certificates    conda-forge::ca-certificates-2024.8.3~ --> pkgs/main::ca-certificates-2024.11.26-h06a4308_0 \n\nThe following packages will be DOWNGRADED:\n\n  libsqlite                               3.47.0-hadc24fc_1 --> 3.46.0-hde9e2c9_0 \n  libzlib                                  1.3.1-hb9d3cd8_2 --> 1.2.13-h4ab18f5_6 \n  python                          3.9.20-h13acc7a_1_cpython --> 3.9.19-h0755675_0_cpython \n\n\n\nDownloading and Extracting Packages:\npython-3.9.19        | 22.7 MB   |                                       |   0% \nintel-openmp-2023.1. | 17.2 MB   |                                       |   0% \u001b[A\n\nlibsqlite-3.46.0     | 845 KB    |                                       |   0% \u001b[A\u001b[A\n\n\nca-certificates-2024 | 131 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nzlib-1.2.13          | 91 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nlibzlib-1.2.13       | 60 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nintel-openmp-2023.1. | 17.2 MB   | ##2                                   |   6% \u001b[A\n\npython-3.9.19        | 22.7 MB   | 4                                     |   1% \u001b[A\u001b[A\n\n\nca-certificates-2024 | 131 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\nca-certificates-2024 | 131 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\n\nlibzlib-1.2.13       | 60 KB     | #########8                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nzlib-1.2.13          | 91 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nzlib-1.2.13          | 91 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\npython-3.9.19        | 22.7 MB   | #####8                                |  16% \u001b[A\n\n\n\n\nlibzlib-1.2.13       | 60 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nlibsqlite-3.46.0     | 845 KB    | ##################################### | 100% \u001b[A\u001b[A\n\nlibsqlite-3.46.0     | 845 KB    | ##################################### | 100% \u001b[A\u001b[A\npython-3.9.19        | 22.7 MB   | ####################5                 |  56% \u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# !source activate env && conda remove pytorch torchvision torchaudio -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:45:15.059734Z","iopub.execute_input":"2024-11-29T02:45:15.060043Z","iopub.status.idle":"2024-11-29T02:45:15.064122Z","shell.execute_reply.started":"2024-11-29T02:45:15.060012Z","shell.execute_reply":"2024-11-29T02:45:15.063363Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"!source activate env && conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:28:40.280699Z","iopub.execute_input":"2024-11-29T16:28:40.281452Z","iopub.status.idle":"2024-11-29T16:31:58.337601Z","shell.execute_reply.started":"2024-11-29T16:28:40.281407Z","shell.execute_reply":"2024-11-29T16:31:58.336444Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Channels:\n - pytorch\n - defaults\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /kaggle/working/miniconda/envs/env\n\n  added / updated specs:\n    - cudatoolkit=11.3\n    - pytorch==1.12.1\n    - torchaudio==0.12.1\n    - torchvision==0.13.1\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    blas-1.0                   |              mkl           6 KB\n    brotli-python-1.0.9        |   py39h6a678d5_8         359 KB\n    certifi-2024.8.30          |   py39h06a4308_0         162 KB\n    cudatoolkit-11.3.1         |       h2bc3f7f_2       549.3 MB\n    ffmpeg-4.3                 |       hf484d3e_0         9.9 MB  pytorch\n    freetype-2.12.1            |       h4a9f257_0         626 KB\n    gmp-6.2.1                  |       h295c915_3         544 KB\n    gnutls-3.6.15              |       he1e5248_0         1.0 MB\n    idna-3.7                   |   py39h06a4308_0         113 KB\n    jpeg-9e                    |       h5eee18b_3         262 KB\n    lame-3.100                 |       h7b6447c_0         323 KB\n    lcms2-2.12                 |       h3be6417_0         312 KB\n    lerc-3.0                   |       h295c915_0         196 KB\n    libdeflate-1.8             |       h7f8727e_5          51 KB\n    libiconv-1.16              |       h5eee18b_3         759 KB\n    libidn2-2.3.4              |       h5eee18b_0         146 KB\n    libpng-1.6.39              |       h5eee18b_0         304 KB\n    libtasn1-4.19.0            |       h5eee18b_0          63 KB\n    libtiff-4.4.0              |       hecacb30_2         526 KB\n    libunistring-0.9.10        |       h27cfd23_0         536 KB\n    libwebp-base-1.3.2         |       h5eee18b_1         425 KB\n    mkl-2023.1.0               |   h213fc3f_46344       171.5 MB\n    mkl-service-2.4.0          |   py39h5eee18b_1          54 KB\n    mkl_fft-1.3.11             |   py39h5eee18b_0         199 KB\n    mkl_random-1.2.8           |   py39h1128e8f_0         319 KB\n    nettle-3.7.3               |       hbbd107a_1         809 KB\n    numpy-2.0.1                |   py39h5f9d8c6_1          11 KB\n    numpy-base-2.0.1           |   py39hb5e798b_1         7.9 MB\n    openh264-2.1.1             |       h4ff587b_0         711 KB\n    openjpeg-2.4.0             |       h9ca470c_2         363 KB\n    pillow-11.0.0              |   py39hfdbf927_0         807 KB\n    pysocks-1.7.1              |   py39h06a4308_0          31 KB\n    pytorch-1.12.1             |py3.9_cuda11.3_cudnn8.3.2_0        1.19 GB  pytorch\n    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n    requests-2.32.3            |   py39h06a4308_1          99 KB\n    tbb-2021.8.0               |       hdb19cb5_0         1.6 MB\n    torchaudio-0.12.1          |       py39_cu113         6.2 MB  pytorch\n    torchvision-0.13.1         |       py39_cu113         7.5 MB  pytorch\n    typing_extensions-4.11.0   |   py39h06a4308_0          58 KB\n    urllib3-2.2.3              |   py39h06a4308_0         180 KB\n    zstd-1.5.2                 |       ha4553b6_0         488 KB\n    ------------------------------------------------------------\n                                           Total:        1.94 GB\n\nThe following NEW packages will be INSTALLED:\n\n  blas               pkgs/main/linux-64::blas-1.0-mkl \n  brotli-python      pkgs/main/linux-64::brotli-python-1.0.9-py39h6a678d5_8 \n  certifi            pkgs/main/linux-64::certifi-2024.8.30-py39h06a4308_0 \n  charset-normalizer pkgs/main/noarch::charset-normalizer-3.3.2-pyhd3eb1b0_0 \n  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2 \n  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0 \n  freetype           pkgs/main/linux-64::freetype-2.12.1-h4a9f257_0 \n  gmp                pkgs/main/linux-64::gmp-6.2.1-h295c915_3 \n  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0 \n  idna               pkgs/main/linux-64::idna-3.7-py39h06a4308_0 \n  jpeg               pkgs/main/linux-64::jpeg-9e-h5eee18b_3 \n  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0 \n  lcms2              pkgs/main/linux-64::lcms2-2.12-h3be6417_0 \n  lerc               pkgs/main/linux-64::lerc-3.0-h295c915_0 \n  libdeflate         pkgs/main/linux-64::libdeflate-1.8-h7f8727e_5 \n  libiconv           pkgs/main/linux-64::libiconv-1.16-h5eee18b_3 \n  libidn2            pkgs/main/linux-64::libidn2-2.3.4-h5eee18b_0 \n  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0 \n  libtasn1           pkgs/main/linux-64::libtasn1-4.19.0-h5eee18b_0 \n  libtiff            pkgs/main/linux-64::libtiff-4.4.0-hecacb30_2 \n  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0 \n  libwebp-base       pkgs/main/linux-64::libwebp-base-1.3.2-h5eee18b_1 \n  lz4-c              pkgs/main/linux-64::lz4-c-1.9.4-h6a678d5_1 \n  mkl                pkgs/main/linux-64::mkl-2023.1.0-h213fc3f_46344 \n  mkl-service        pkgs/main/linux-64::mkl-service-2.4.0-py39h5eee18b_1 \n  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.11-py39h5eee18b_0 \n  mkl_random         pkgs/main/linux-64::mkl_random-1.2.8-py39h1128e8f_0 \n  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1 \n  numpy              pkgs/main/linux-64::numpy-2.0.1-py39h5f9d8c6_1 \n  numpy-base         pkgs/main/linux-64::numpy-base-2.0.1-py39hb5e798b_1 \n  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0 \n  openjpeg           pkgs/main/linux-64::openjpeg-2.4.0-h9ca470c_2 \n  pillow             pkgs/main/linux-64::pillow-11.0.0-py39hfdbf927_0 \n  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py39h06a4308_0 \n  pytorch            pytorch/linux-64::pytorch-1.12.1-py3.9_cuda11.3_cudnn8.3.2_0 \n  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda \n  requests           pkgs/main/linux-64::requests-2.32.3-py39h06a4308_1 \n  tbb                pkgs/main/linux-64::tbb-2021.8.0-hdb19cb5_0 \n  torchaudio         pytorch/linux-64::torchaudio-0.12.1-py39_cu113 \n  torchvision        pytorch/linux-64::torchvision-0.13.1-py39_cu113 \n  typing_extensions  pkgs/main/linux-64::typing_extensions-4.11.0-py39h06a4308_0 \n  urllib3            pkgs/main/linux-64::urllib3-2.2.3-py39h06a4308_0 \n  zstd               pkgs/main/linux-64::zstd-1.5.2-ha4553b6_0 \n\n\n\nDownloading and Extracting Packages:\npytorch-1.12.1       | 1.19 GB   |                                       |   0% \ncudatoolkit-11.3.1   | 549.3 MB  |                                       |   0% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  |                                       |   0% \u001b[A\u001b[A\n\n\nffmpeg-4.3           | 9.9 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nnumpy-base-2.0.1     | 7.9 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\ntorchvision-0.13.1   | 7.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\ntorchaudio-0.12.1    | 6.2 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\ntbb-2021.8.0         | 1.6 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\ngnutls-3.6.15        | 1.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nnettle-3.7.3         | 809 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npillow-11.0.0        | 807 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nlibiconv-1.16        | 759 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenh264-2.1.1       | 711 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nfreetype-2.12.1      | 626 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\ngmp-6.2.1            | 544 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibunistring-0.9.10  | 536 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibtiff-4.4.0        | 526 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzstd-1.5.2           | 488 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.3.2   | 425 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 363 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrotli-python-1.0.9  | 359 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlame-3.100           | 323 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmkl_random-1.2.8     | 319 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   |                                       |   0% [A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | 1                                     |   0% \u001b[A\n\n\n\nnumpy-base-2.0.1     | 7.9 MB    | ######3                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\nffmpeg-4.3           | 9.9 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 1                                     |   0% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | 3                                     |   1% \u001b[A\n\n\n\nnumpy-base-2.0.1     | 7.9 MB    | ########################7             |  67% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\nffmpeg-4.3           | 9.9 MB    | #######7                              |  21% \u001b[A\u001b[A\u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 2                                     |   1% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | 6                                     |   2% \u001b[A\n\n\nffmpeg-4.3           | 9.9 MB    | ###################7                  |  53% \u001b[A\u001b[A\u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 3                                     |   1% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | 8                                     |   2% \u001b[A\n\n\nffmpeg-4.3           | 9.9 MB    | ################################2     |  87% \u001b[A\u001b[A\u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 4                                     |   1% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #1                                    |   3% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 5                                     |   1% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #3                                    |   4% \u001b[A\n\n\n\n\ntorchvision-0.13.1   | 7.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 6                                     |   2% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #5                                    |   4% \u001b[A\n\n\n\n\ntorchvision-0.13.1   | 7.5 MB    | #######6                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 7                                     |   2% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #8                                    |   5% \u001b[A\n\n\n\n\ntorchvision-0.13.1   | 7.5 MB    | #####################8                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | #####                                 |  14% \u001b[A\u001b[A\n\n\n\n\n\ntorchaudio-0.12.1    | 6.2 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ##                                    |   6% \u001b[A\n\n\n\n\n\ntorchaudio-0.12.1    | 6.2 MB    | ###########                           |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\npytorch-1.12.1       | 1.19 GB   | 9                                     |   3% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ##2                                   |   6% \u001b[A\n\n\n\n\n\ntorchaudio-0.12.1    | 6.2 MB    | #########################9            |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ######2                               |  17% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #                                     |   3% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | #1                                    |   3% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ##6                                   |   7% \u001b[A\n\n\n\n\n\n\ntbb-2021.8.0         | 1.6 MB    | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | #######5                              |  20% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #2                                    |   3% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ########1                             |  22% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\nnettle-3.7.3         | 809 KB    | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\ngnutls-3.6.15        | 1.0 MB    | 5                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #2                                    |   3% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ########7                             |  24% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npillow-11.0.0        | 807 KB    | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #3                                    |   4% \u001b[A\n\n\n\n\n\n\n\n\n\n\nlibiconv-1.16        | 759 KB    | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | #########3                            |  25% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenh264-2.1.1       | 711 KB    | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nfreetype-2.12.1      | 626 KB    | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #4                                    |   4% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\ngmp-6.2.1            | 544 KB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | #########9                            |  27% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibunistring-0.9.10  | 536 KB    | #1                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #5                                    |   4% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibtiff-4.4.0        | 526 KB    | #1                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzstd-1.5.2           | 488 KB    | #2                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ##########5                           |  28% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.3.2   | 425 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #5                                    |   4% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 363 KB    | #6                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ###########1                          |  30% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrotli-python-1.0.9  | 359 KB    | #6                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | #6                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ###9                                  |  11% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmkl_random-1.2.8     | 319 KB    | #8                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ###########7                          |  32% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | #7                                    |   5% [A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ####1                                 |  11% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | #8                                    |   5% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ####3                                 |  12% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | #9                                    |   5% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ####4                                 |  12% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##                                    |   5% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ####7                                 |  13% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##                                    |   6% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ####9                                 |  13% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##1                                   |   6% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #####1                                |  14% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##2                                   |   6% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #####3                                |  15% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##3                                   |   6% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #####5                                |  15% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##4                                   |   7% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #####8                                |  16% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##5                                   |   7% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ######                                |  16% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##6                                   |   7% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ######2                               |  17% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##7                                   |   7% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ######4                               |  18% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##8                                   |   8% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ######7                               |  18% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ##9                                   |   8% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ######9                               |  19% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###                                   |   8% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #######1                              |  19% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###1                                  |   9% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #######3                              |  20% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###2                                  |   9% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #######6                              |  21% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###3                                  |   9% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #######8                              |  21% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###4                                  |   9% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ########                              |  22% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###5                                  |  10% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ########3                             |  23% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###6                                  |  10% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ########5                             |  23% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###7                                  |  10% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ########7                             |  24% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ###8                                  |  10% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #########                             |  24% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ###########################5          |  74% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ###9                                  |  11% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ############################2         |  76% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ####                                  |  11% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | #############################         |  78% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ####1                                 |  11% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | #############################7        |  80% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ####2                                 |  11% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ##############################4       |  82% \u001b[A\u001b[A\n\n\npytorch-1.12.1       | 1.19 GB   | ####3                                 |  12% \u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ##########1                           |  27% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ####4                                 |  12% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ##########3                           |  28% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ###############################9      |  86% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ####5                                 |  12% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ################################7     |  89% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ####6                                 |  12% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | #################################4    |  90% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ####7                                 |  13% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ##################################2   |  92% \u001b[A\u001b[A\n\n\n\n\ntorchvision-0.13.1   | 7.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ####8                                 |  13% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ####9                                 |  13% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ###########4                          |  31% \u001b[A\n\nmkl-2023.1.0         | 171.5 MB  | ###################################5  |  96% \u001b[A\u001b[A\n\n\n\n\n\n\ntbb-2021.8.0         | 1.6 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\ntbb-2021.8.0         | 1.6 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #####                                 |  14% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | #####                                 |  14% \u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ###########9                          |  32% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | #####1                                |  14% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #####3                                |  14% \u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ############3                         |  33% \u001b[A\n\n\n\n\n\n\n\n\nnettle-3.7.3         | 809 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | #####4                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #####5                                |  15% \u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ############8                         |  35% \u001b[A\n\n\n\n\n\n\n\ngnutls-3.6.15        | 1.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | #####6                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #############                         |  35% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #####7                                |  16% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #####8                                |  16% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #####9                                |  16% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ######                                |  16% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ######1                               |  17% \u001b[A\n\n\n\nnumpy-base-2.0.1     | 7.9 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ######2                               |  17% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ######3                               |  17% \u001b[A\n\n\n\n\n\n\n\n\n\npillow-11.0.0        | 807 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npillow-11.0.0        | 807 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ######4                               |  17% \u001b[A\n\n\n\n\n\n\n\n\n\n\nlibiconv-1.16        | 759 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nlibiconv-1.16        | 759 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ######5                               |  18% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenh264-2.1.1       | 711 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenh264-2.1.1       | 711 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ######6                               |  18% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\ngmp-6.2.1            | 544 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | ######7                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ###############7                      |  43% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nfreetype-2.12.1      | 626 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nfreetype-2.12.1      | 626 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ######8                               |  19% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibunistring-0.9.10  | 536 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibunistring-0.9.10  | 536 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ################2                     |  44% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibtiff-4.4.0        | 526 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | ######9                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzstd-1.5.2           | 488 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzstd-1.5.2           | 488 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #######                               |  19% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.3.2   | 425 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.3.2   | 425 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 363 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 363 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #######1                              |  19% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrotli-python-1.0.9  | 359 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrotli-python-1.0.9  | 359 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #######2                              |  20% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlame-3.100           | 323 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | #######3                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #################2                    |  47% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmkl_random-1.2.8     | 319 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmkl_random-1.2.8     | 319 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | #######4                              |  20% [A\u001b[A\u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | #######5                              |  21% \u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #################6                    |  48% \u001b[A\n\n\n\n\n\ntorchaudio-0.12.1    | 6.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\npytorch-1.12.1       | 1.19 GB   | #######6                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | #################9                    |  48% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #######7                              |  21% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #######9                              |  22% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ########                              |  22% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ########2                             |  22% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ########4                             |  23% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ########7                             |  24% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #########                             |  24% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #########3                            |  25% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #########6                            |  26% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #########9                            |  27% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ##########2                           |  28% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ##########4                           |  28% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ##########8                           |  29% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ###########1                          |  30% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ###########3                          |  31% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ###########6                          |  31% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ###########9                          |  32% \u001b[A\ncudatoolkit-11.3.1   | 549.3 MB  | ############################1         |  76% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ############4                         |  34% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ############6                         |  34% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ############9                         |  35% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #############1                        |  36% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #############4                        |  36% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #############6                        |  37% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #############9                        |  38% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ##############1                       |  38% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ##############4                       |  39% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ##############6                       |  40% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ##############9                       |  40% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ###############1                      |  41% \u001b[A\npytorch-1.12.1       | 1.19 GB   | ###############4                      |  42% \u001b[A\npytorch-1.12.1       | 1.19 GB   | #####################3                |  58% \u001b[A\n\npytorch-1.12.1       | 1.19 GB   | ####################################7 |  99% \u001b[A\u001b[A\npytorch-1.12.1       | 1.19 GB   | ##################################### | 100% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: / By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n\ndone\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# !source activate env && pip uninstall -y huggingface_hub diffusers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:48:27.658834Z","iopub.execute_input":"2024-11-29T02:48:27.659134Z","iopub.status.idle":"2024-11-29T02:48:27.663368Z","shell.execute_reply.started":"2024-11-29T02:48:27.659105Z","shell.execute_reply":"2024-11-29T02:48:27.662421Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# !source activate env && pip install diffusers==0.10.2 huggingface_hub==0.14.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:48:27.664497Z","iopub.execute_input":"2024-11-29T02:48:27.664771Z","iopub.status.idle":"2024-11-29T02:48:27.677505Z","shell.execute_reply.started":"2024-11-29T02:48:27.664747Z","shell.execute_reply":"2024-11-29T02:48:27.676893Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# !source activate env && pip uninstall -y diffusers huggingface_hub accelerate transformers torch torchvision torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:48:27.678492Z","iopub.execute_input":"2024-11-29T02:48:27.678750Z","iopub.status.idle":"2024-11-29T02:48:27.688739Z","shell.execute_reply.started":"2024-11-29T02:48:27.678710Z","shell.execute_reply":"2024-11-29T02:48:27.688059Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# !source activate env && pip install diffusers==0.10.0 huggingface_hub==0.11.1 accelerate==0.12.0 transformers==4.25.1 torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:48:27.689692Z","iopub.execute_input":"2024-11-29T02:48:27.690011Z","iopub.status.idle":"2024-11-29T02:48:27.698466Z","shell.execute_reply.started":"2024-11-29T02:48:27.689964Z","shell.execute_reply":"2024-11-29T02:48:27.697863Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!source activate env && pip uninstall -y numpy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:31:58.339113Z","iopub.execute_input":"2024-11-29T16:31:58.339415Z","iopub.status.idle":"2024-11-29T16:32:00.434578Z","shell.execute_reply.started":"2024-11-29T16:31:58.339385Z","shell.execute_reply":"2024-11-29T16:32:00.433625Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 2.0.1\nUninstalling numpy-2.0.1:\n  Successfully uninstalled numpy-2.0.1\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!source activate env && pip install numpy==1.23\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:00.436046Z","iopub.execute_input":"2024-11-29T16:32:00.436421Z","iopub.status.idle":"2024-11-29T16:32:06.193419Z","shell.execute_reply.started":"2024-11-29T16:32:00.436380Z","shell.execute_reply":"2024-11-29T16:32:06.192363Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.23\n  Downloading numpy-1.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\nDownloading numpy-1.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\nSuccessfully installed numpy-1.23.0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!source activate env && pip install ipykernel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:06.194935Z","iopub.execute_input":"2024-11-29T16:32:06.195316Z","iopub.status.idle":"2024-11-29T16:32:11.303264Z","shell.execute_reply.started":"2024-11-29T16:32:06.195264Z","shell.execute_reply":"2024-11-29T16:32:11.302319Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting ipykernel\n  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: comm>=0.1.1 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipykernel) (0.2.2)\nCollecting debugpy>=1.6.5 (from ipykernel)\n  Downloading debugpy-1.8.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: ipython>=7.23.1 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipykernel) (8.18.1)\nCollecting jupyter-client>=6.1.12 (from ipykernel)\n  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\nCollecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: matplotlib-inline>=0.1 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipykernel) (0.1.7)\nCollecting nest-asyncio (from ipykernel)\n  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: packaging in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipykernel) (24.2)\nRequirement already satisfied: psutil in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipykernel) (6.1.0)\nCollecting pyzmq>=24 (from ipykernel)\n  Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\nCollecting tornado>=6.1 (from ipykernel)\n  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nRequirement already satisfied: traitlets>=5.4.0 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipykernel) (5.14.3)\nRequirement already satisfied: decorator in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (3.0.48)\nRequirement already satisfied: pygments>=2.4.0 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\nRequirement already satisfied: stack-data in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\nRequirement already satisfied: typing-extensions in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.11.0)\nRequirement already satisfied: exceptiongroup in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\nRequirement already satisfied: pexpect>4.3 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\nRequirement already satisfied: importlib-metadata>=4.8.3 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\nCollecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: zipp>=3.20 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.21.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\nRequirement already satisfied: wcwidth in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\nRequirement already satisfied: six>=1.5 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\nRequirement already satisfied: executing>=1.2.0 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.1.0)\nRequirement already satisfied: asttokens>=2.1.0 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\nRequirement already satisfied: pure-eval in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\nDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\nDownloading debugpy-1.8.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\nDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\nDownloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (862 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m862.1/862.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\nDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\nDownloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\nInstalling collected packages: tornado, pyzmq, platformdirs, nest-asyncio, debugpy, jupyter-core, jupyter-client, ipykernel\nSuccessfully installed debugpy-1.8.9 ipykernel-6.29.5 jupyter-client-8.6.3 jupyter-core-5.7.2 nest-asyncio-1.6.0 platformdirs-4.3.6 pyzmq-26.2.0 tornado-6.4.2\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!source activate env && pip uninstall -y pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:11.307027Z","iopub.execute_input":"2024-11-29T16:32:11.307541Z","iopub.status.idle":"2024-11-29T16:32:13.082378Z","shell.execute_reply.started":"2024-11-29T16:32:11.307492Z","shell.execute_reply":"2024-11-29T16:32:13.081225Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pillow 11.0.0\nUninstalling pillow-11.0.0:\n  Successfully uninstalled pillow-11.0.0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!source activate env && pip install pillow==9.5.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:13.083955Z","iopub.execute_input":"2024-11-29T16:32:13.084348Z","iopub.status.idle":"2024-11-29T16:32:16.170097Z","shell.execute_reply.started":"2024-11-29T16:32:13.084300Z","shell.execute_reply":"2024-11-29T16:32:16.169199Z"}},"outputs":[{"name":"stdout","text":"Collecting pillow==9.5.0\n  Downloading Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nDownloading Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pillow\nSuccessfully installed pillow-9.5.0\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!source activate env && pip install sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:16.171487Z","iopub.execute_input":"2024-11-29T16:32:16.171797Z","iopub.status.idle":"2024-11-29T16:32:18.749429Z","shell.execute_reply.started":"2024-11-29T16:32:16.171767Z","shell.execute_reply":"2024-11-29T16:32:18.748542Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting sentencepiece\n  Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nDownloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentencepiece\nSuccessfully installed sentencepiece-0.2.0\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!source activate env && pip uninstall -y diffusers huggingface_hub transformers accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:18.750734Z","iopub.execute_input":"2024-11-29T16:32:18.751028Z","iopub.status.idle":"2024-11-29T16:32:20.960787Z","shell.execute_reply.started":"2024-11-29T16:32:18.751001Z","shell.execute_reply":"2024-11-29T16:32:20.959843Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found existing installation: diffusers 0.10.0\nUninstalling diffusers-0.10.0:\n  Successfully uninstalled diffusers-0.10.0\nFound existing installation: huggingface-hub 0.26.3\nUninstalling huggingface-hub-0.26.3:\n  Successfully uninstalled huggingface-hub-0.26.3\nFound existing installation: transformers 4.46.3\nUninstalling transformers-4.46.3:\n  Successfully uninstalled transformers-4.46.3\nFound existing installation: accelerate 1.1.1\nUninstalling accelerate-1.1.1:\n  Successfully uninstalled accelerate-1.1.1\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!source activate env && pip install diffusers==0.10.2 huggingface_hub==0.15.1 transformers==4.28.0 accelerate==0.12.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:20.962516Z","iopub.execute_input":"2024-11-29T16:32:20.962872Z","iopub.status.idle":"2024-11-29T16:32:29.099195Z","shell.execute_reply.started":"2024-11-29T16:32:20.962841Z","shell.execute_reply":"2024-11-29T16:32:29.098256Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting diffusers==0.10.2\n  Downloading diffusers-0.10.2-py3-none-any.whl.metadata (31 kB)\nCollecting huggingface_hub==0.15.1\n  Downloading huggingface_hub-0.15.1-py3-none-any.whl.metadata (8.0 kB)\nCollecting transformers==4.28.0\n  Downloading transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\nCollecting accelerate==0.12.0\n  Downloading accelerate-0.12.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: importlib-metadata in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from diffusers==0.10.2) (8.5.0)\nRequirement already satisfied: filelock in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from diffusers==0.10.2) (3.16.1)\nRequirement already satisfied: numpy in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from diffusers==0.10.2) (1.23.0)\nRequirement already satisfied: regex!=2019.12.17 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from diffusers==0.10.2) (2024.11.6)\nRequirement already satisfied: requests in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from diffusers==0.10.2) (2.32.3)\nRequirement already satisfied: Pillow in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from diffusers==0.10.2) (9.5.0)\nRequirement already satisfied: fsspec in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from huggingface_hub==0.15.1) (2024.10.0)\nRequirement already satisfied: tqdm>=4.42.1 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from huggingface_hub==0.15.1) (4.67.1)\nRequirement already satisfied: pyyaml>=5.1 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from huggingface_hub==0.15.1) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from huggingface_hub==0.15.1) (4.11.0)\nRequirement already satisfied: packaging>=20.9 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from huggingface_hub==0.15.1) (24.2)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: psutil in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from accelerate==0.12.0) (6.1.0)\nRequirement already satisfied: torch>=1.4.0 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from accelerate==0.12.0) (1.12.1)\nRequirement already satisfied: zipp>=3.20 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from importlib-metadata->diffusers==0.10.2) (3.21.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from requests->diffusers==0.10.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from requests->diffusers==0.10.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from requests->diffusers==0.10.2) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from requests->diffusers==0.10.2) (2024.8.30)\nDownloading diffusers-0.10.2-py3-none-any.whl (503 kB)\nDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\nDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-0.12.0-py3-none-any.whl (143 kB)\nDownloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, huggingface_hub, accelerate, transformers, diffusers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\nSuccessfully installed accelerate-0.12.0 diffusers-0.10.2 huggingface_hub-0.15.1 tokenizers-0.13.3 transformers-4.28.0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!source activate env && pip install protobuf==3.20.3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:32:29.100571Z","iopub.execute_input":"2024-11-29T16:32:29.100862Z","iopub.status.idle":"2024-11-29T16:32:31.930846Z","shell.execute_reply.started":"2024-11-29T16:32:29.100834Z","shell.execute_reply":"2024-11-29T16:32:31.929936Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\nDownloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\nSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"!rm -rf /kaggle/working/PnPInversion/output/edit-friendly-inversion+p2p/annotation_images/0_random_140/000000000000.jpg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T23:21:58.562539Z","iopub.execute_input":"2024-11-29T23:21:58.563138Z","iopub.status.idle":"2024-11-29T23:21:59.568809Z","shell.execute_reply.started":"2024-11-29T23:21:58.563104Z","shell.execute_reply":"2024-11-29T23:21:59.567827Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"Stable diffusion change only","metadata":{}},{"cell_type":"code","source":"%%writefile \"/kaggle/working/PnPInversion/run_editing_edit_friendly_p2p.py\"\n# \n\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers import DDIMScheduler\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom models.p2p.scheduler_dev import DDIMSchedulerDev\nimport json\nimport random\nimport argparse\nfrom torch import autocast, inference_mode\nfrom torch.nn import Linear, Module\nfrom utils.utils import load_512,txt_draw\nfrom models.edit_friendly_ddm.inversion_utils import inversion_forward_process, inversion_reverse_process\nfrom models.edit_friendly_ddm.ptp_classes import AttentionReplace,AttentionRefine,AttentionStore\nfrom models.edit_friendly_ddm.ptp_utils import register_attention_control\nfrom sklearn.decomposition import PCA\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, AutoModel\nclass CustomLlamaModel(torch.nn.Module):\n    def __init__(self):\n        super(CustomLlamaModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n        # print(f'size: {self.base_model.config.hidden_size}')\n        self.projection = torch.nn.Linear(self.base_model.config.hidden_size, 768) #\n        \n\n    def forward(self, **kwargs):\n        # Pass inputs through the base model\n        outputs = self.base_model(**kwargs)\n        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n        \n        # Project the embeddings to the desired dimension\n        projected_embeddings = self.projection(last_hidden_state)  # Shape: (batch_size, seq_len, target_dim)\n        return projected_embeddings\n\n        \ndef mask_decode(encoded_mask,image_shape=[512,512]):\n    length=image_shape[0]*image_shape[1]\n    mask_array=np.zeros((length,))\n    \n    for i in range(0,len(encoded_mask),2):\n        splice_len=min(encoded_mask[i+1],length-encoded_mask[i])\n        for j in range(splice_len):\n            mask_array[encoded_mask[i]+j]=1\n            \n    mask_array=mask_array.reshape(image_shape[0], image_shape[1])\n    # to avoid annotation errors in boundary\n    mask_array[0,:]=1\n    mask_array[-1,:]=1\n    mask_array[:,0]=1\n    mask_array[:,-1]=1\n            \n    return mask_array\n\n\ndef setup_seed(seed=1234):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nimage_save_paths={\n    \"edit-friendly-inversion+p2p\":\"edit-friendly-inversion+p2p\",\n    }\n\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device(\n    'cpu')\nNUM_DDIM_STEPS = 50\nmodel_id=\"CompVis/stable-diffusion-v1-4\"\n#ldm_stable = StableDiffusionPipeline.from_pretrained(\n#    model_id).to(device)\n#ldm_stable.scheduler = DDIMScheduler.from_config(model_id, subfolder = \"scheduler\")\n#ldm_stable.scheduler.set_timesteps(NUM_DDIM_STEPS)\n\nprint(\"Starting Stable Diffusion...\")\n# model_id = \"stabilityai/stable-diffusion-2-1-base\"\nldm_stable = StableDiffusionPipeline.from_pretrained(model_id).to(device)\nldm_stable.scheduler = DDIMScheduler.from_config(model_id, subfolder=\"scheduler\")\nldm_stable.scheduler.set_timesteps(NUM_DDIM_STEPS)\n\ntarget_dim = 1024\nmodel_name = \"openlm-research/open_llama_3b_v2\"\n\ncustom_model = CustomLlamaModel()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\nETA=1\nSKIP=12\n\n\ndef edit_image_EF(edit_method,\n                  image_path,\n                    prompt_src,\n                    prompt_tar,\n                    source_guidance_scale=1,\n                    target_guidance_scale=7.5,cross_replace_steps=0.4,\n                    self_replace_steps=0.6\n                    ):\n    if edit_method==\"edit-friendly-inversion+p2p\":\n        image_gt = load_512(image_path)\n        \n        image_gt = torch.from_numpy(image_gt).float() / 127.5 - 1\n        image_gt = image_gt.permute(2, 0, 1).unsqueeze(0).to(device)\n        with autocast(\"cuda\"), inference_mode():\n            w0 = (ldm_stable.vae.encode(image_gt).latent_dist.mode() * 0.18215).float()\n            \n        controller = AttentionStore()\n        register_attention_control(ldm_stable, controller)\n            \n        wt, zs, wts = inversion_forward_process(tokenizer, custom_model, ldm_stable, w0, etas=ETA, prompt=prompt_src, cfg_scale=source_guidance_scale, prog_bar=True, num_inference_steps=NUM_DDIM_STEPS)\n        \n        controller = AttentionStore()\n        register_attention_control(ldm_stable, controller)\n        \n        x0_reconstruct, _ = inversion_reverse_process(tokenizer, custom_model, ldm_stable, xT=wts[NUM_DDIM_STEPS-SKIP], etas=ETA, prompts=[prompt_tar], cfg_scales=[target_guidance_scale], prog_bar=True, zs=zs[:(NUM_DDIM_STEPS-SKIP)], controller=controller)\n\n        cfg_scale_list = [source_guidance_scale, target_guidance_scale]\n        prompts = [prompt_src, prompt_tar]\n        if (len(prompt_src.split(\" \")) == len(prompt_tar.split(\" \"))):\n            controller = AttentionReplace(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, model=ldm_stable)\n        else:\n            # Should use Refine for target prompts with different number of tokens\n            controller = AttentionRefine(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, model=ldm_stable)\n\n        register_attention_control(ldm_stable, controller)\n        w0, _ = inversion_reverse_process(tokenizer, custom_model, ldm_stable, xT=wts[NUM_DDIM_STEPS-SKIP], etas=ETA, prompts=prompts, cfg_scales=cfg_scale_list, prog_bar=True, zs=zs[:(NUM_DDIM_STEPS-SKIP)], controller=controller)\n        with autocast(\"cuda\"), inference_mode():\n            x0_dec = ldm_stable.vae.decode(1 / 0.18215 * w0[1].unsqueeze(0)).sample\n            x0_reconstruct_edit = ldm_stable.vae.decode(1 / 0.18215 * w0[0].unsqueeze(0)).sample\n            x0_reconstruct = ldm_stable.vae.decode(1 / 0.18215 * x0_reconstruct[0].unsqueeze(0)).sample\n            \n        image_instruct = txt_draw(f\"source prompt: {prompt_src}\\ntarget prompt: {prompt_tar}\")\n            \n        return Image.fromarray(np.concatenate(\n                                            (\n                                                image_instruct,\n                                                np.uint8((np.array(image_gt[0].permute(1,2,0).cpu().detach())/2+ 0.5)*255),\n                                                np.uint8((np.array(x0_reconstruct_edit[0].permute(1,2,0).cpu().detach())/2+ 0.5)*255),\n                                                np.uint8((np.array(x0_dec[0].permute(1,2,0).cpu().detach())/2+ 0.5)*255)\n                                            ),\n                                            1\n                                            )\n                            )\n    else:\n        raise NotImplementedError(f\"No edit method named {edit_method}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rerun_exist_images', action= \"store_true\") # rerun existing images\n    parser.add_argument('--data_path', type=str, default=\"data\") # the editing category that needed to run\n    parser.add_argument('--output_path', type=str, default=\"output\") # the editing category that needed to run\n    parser.add_argument('--edit_category_list', nargs = '+', type=str, default=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]) # the editing category that needed to run\n    parser.add_argument('--edit_method_list', nargs = '+', type=str, default=[\"edit-friendly-inversion+p2p\"]) # the editing methods that needed to run\n    args = parser.parse_args()\n    \n    rerun_exist_images=args.rerun_exist_images\n    data_path=args.data_path\n    output_path=args.output_path\n    edit_category_list=args.edit_category_list\n    edit_method_list=args.edit_method_list\n    \n    with open(f\"{data_path}/mapping_file.json\", \"r\") as f:\n        editing_instruction = json.load(f)\n    \n    for key, item in editing_instruction.items():\n        \n        if item[\"editing_type_id\"] not in edit_category_list:\n            continue\n        \n        original_prompt = item[\"original_prompt\"].replace(\"[\", \"\").replace(\"]\", \"\")\n        editing_prompt = item[\"editing_prompt\"].replace(\"[\", \"\").replace(\"]\", \"\")\n        image_path = os.path.join(f\"{data_path}/annotation_images\", item[\"image_path\"])\n        editing_instruction = item[\"editing_instruction\"]\n        blended_word = item[\"blended_word\"].split(\" \") if item[\"blended_word\"] != \"\" else []\n        mask = Image.fromarray(np.uint8(mask_decode(item[\"mask\"])[:,:,np.newaxis].repeat(3,2))).convert(\"L\")\n\n        for edit_method in edit_method_list:\n            present_image_save_path=image_path.replace(data_path, os.path.join(output_path,image_save_paths[edit_method]))\n            if ((not os.path.exists(present_image_save_path)) or rerun_exist_images):\n                print(f\"editing image [{image_path}] with [{edit_method}]\")\n                setup_seed()\n                torch.cuda.empty_cache()\n                edited_image = edit_image_EF(\n                        edit_method=edit_method,\n                        image_path=image_path,\n                        prompt_src=original_prompt,\n                        prompt_tar=editing_prompt,\n                        source_guidance_scale=1,\n                        target_guidance_scale=7.5,\n                        cross_replace_steps=0.4,\n                        self_replace_steps=0.6\n                        )\n                        \n                if not os.path.exists(os.path.dirname(present_image_save_path)):\n                    os.makedirs(os.path.dirname(present_image_save_path))\n                edited_image.save(present_image_save_path)\n                \n                print(f\"finish\")\n                \n            else:\n                print(f\"skip image [{image_path}] with [{edit_method}]\")\n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T00:36:40.904023Z","iopub.execute_input":"2024-11-30T00:36:40.904401Z","iopub.status.idle":"2024-11-30T00:36:40.914811Z","shell.execute_reply.started":"2024-11-30T00:36:40.904372Z","shell.execute_reply":"2024-11-30T00:36:40.913953Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/PnPInversion/run_editing_edit_friendly_p2p.py\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"Encoder changes","metadata":{}},{"cell_type":"code","source":"%%writefile \"/kaggle/working/PnPInversion/models/edit_friendly_ddm/inversion_utils.py\"\n# %load \"/kaggle/working/PnPInversion/models/edit_friendly_ddm/inversion_utils.py\"\nimport torch\nimport os\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, AutoModel\n# class CustomLlamaModel(torch.nn.Module):\n#     def __init__(self):\n#         super(CustomLlamaModel, self).__init__()\n#         self.base_model = AutoModel.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n#         self.projection = torch.nn.Linear(self.base_model.config.hidden_size, 1024)\n\n#     def forward(self, **kwargs):\n#         # Pass inputs through the base model\n#         outputs = self.base_model(**kwargs)\n#         last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n        \n#         # Project the embeddings to the desired dimension\n#         projected_embeddings = self.projection(last_hidden_state)  # Shape: (batch_size, seq_len, target_dim)\n#         return projected_embeddings\n        \ndef load_real_image(folder = \"data/\", img_name = None, idx = 0, img_size=512, device='cuda'):\n    from ddm_inversion.utils import pil_to_tensor\n    from PIL import Image\n    from glob import glob\n    if img_name is not None:\n        path = os.path.join(folder, img_name)\n    else:\n        path = glob(folder + \"*\")[idx]\n\n    img = Image.open(path).resize((img_size,\n                                    img_size))\n\n    img = pil_to_tensor(img).to(device)\n\n    if img.shape[1]== 4:\n        img = img[:,:3,:,:]\n    return img\n\ndef mu_tilde(model, xt,x0, timestep):\n    \"mu_tilde(x_t, x_0) DDPM paper eq. 7\"\n    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n    alpha_t = model.scheduler.alphas[timestep]\n    beta_t = 1 - alpha_t \n    alpha_bar = model.scheduler.alphas_cumprod[timestep]\n    return ((alpha_prod_t_prev ** 0.5 * beta_t) / (1-alpha_bar)) * x0 +  ((alpha_t**0.5 *(1-alpha_prod_t_prev)) / (1- alpha_bar))*xt\n\ndef sample_xts_from_x0(model, x0, num_inference_steps=50):\n    \"\"\"\n    Samples from P(x_1:T|x_0)\n    \"\"\"\n    # torch.manual_seed(43256465436)\n    alpha_bar = model.scheduler.alphas_cumprod\n    sqrt_one_minus_alpha_bar = (1-alpha_bar) ** 0.5\n    alphas = model.scheduler.alphas\n    betas = 1 - alphas\n    variance_noise_shape = (\n            num_inference_steps,\n            model.unet.in_channels, \n            model.unet.sample_size,\n            model.unet.sample_size)\n    \n    timesteps = model.scheduler.timesteps.to(model.device)\n    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n    xts = torch.zeros((num_inference_steps+1,model.unet.in_channels, model.unet.sample_size, model.unet.sample_size)).to(x0.device)\n    xts[0] = x0\n    for t in reversed(timesteps):\n        idx = num_inference_steps-t_to_idx[int(t)]\n        xts[idx] = x0 * (alpha_bar[t] ** 0.5) +  torch.randn_like(x0) * sqrt_one_minus_alpha_bar[t]\n\n\n    return xts\n\n\ndef encode_text(model, prompts, tokenizer, custom_model):\n    # print(f\"prompts: {prompts}\")\n    # target_dim = 768\n    # model_name = \"openlm-research/open_llama_3b_v2\"\n\n    # custom_model = CustomLlamaModel()\n\n\n    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # if tokenizer.pad_token is None:\n    #         tokenizer.pad_token = tokenizer.eos_token\n\n\n    text_input = tokenizer(\n        prompts,\n        padding=\"max_length\",\n        max_length=77, \n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    # print(text_input)\n    if \"token_type_ids\" in text_input:\n        del text_input[\"token_type_ids\"]\n        \n    with torch.no_grad():\n        text_encoding = custom_model(**text_input).to(model.device)\n        # print(f\"text_encoding: {text_encoding.shape}\")\n    # print(\"Encode_text completed.\")\n    return text_encoding\n\ndef forward_step(model, model_output, timestep, sample):\n    next_timestep = min(model.scheduler.config.num_train_timesteps - 2,\n                        timestep + model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps)\n\n    # 2. compute alphas, betas\n    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n    # alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep] if next_ltimestep >= 0 else self.scheduler.final_alpha_cumprod\n\n    beta_prod_t = 1 - alpha_prod_t\n\n    # 3. compute predicted original sample from predicted noise also called\n    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n\n    # 5. TODO: simple noising implementatiom\n    next_sample = model.scheduler.add_noise(pred_original_sample,\n                                    model_output,\n                                    torch.LongTensor([next_timestep]))\n    return next_sample\n\n\ndef get_variance(model, timestep): #, prev_timestep):\n    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n    return variance\n\ndef inversion_forward_process(tokenizer, custom_model, model, x0, \n                            etas = None,    \n                            prog_bar = False,\n                            prompt = \"\",\n                            cfg_scale = 3.5,\n                            num_inference_steps=50, eps = None):\n\n    if not prompt==\"\":\n        text_embeddings = encode_text(model, prompt, tokenizer, custom_model)\n    uncond_embedding = encode_text(model, \"\", tokenizer, custom_model)\n    timesteps = model.scheduler.timesteps.to(model.device)\n    variance_noise_shape = (\n        num_inference_steps,\n        model.unet.in_channels, \n        model.unet.sample_size,\n        model.unet.sample_size)\n    if etas is None or (type(etas) in [int, float] and etas == 0):\n        eta_is_zero = True\n        zs = None\n    else:\n        eta_is_zero = False\n        if type(etas) in [int, float]: etas = [etas]*model.scheduler.num_inference_steps\n        xts = sample_xts_from_x0(model, x0, num_inference_steps=num_inference_steps)\n        alpha_bar = model.scheduler.alphas_cumprod\n        zs = torch.zeros(size=variance_noise_shape, device=model.device)\n    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n    xt = x0\n    op = timesteps\n\n    for t in op:\n        # idx = t_to_idx[int(t)]\n        idx = num_inference_steps-t_to_idx[int(t)]-1\n        # 1. predict noise residual\n        if not eta_is_zero:\n            xt = xts[idx+1][None]\n            # xt = xts_cycle[idx+1][None]\n                    \n        with torch.no_grad():\n            out = model.unet.forward(xt, timestep =  t, encoder_hidden_states = uncond_embedding)\n            if not prompt==\"\":\n                cond_out = model.unet.forward(xt, timestep=t, encoder_hidden_states = text_embeddings)\n\n        if not prompt==\"\":\n            ## classifier free guidance\n            noise_pred = out.sample + cfg_scale * (cond_out.sample - out.sample)\n        else:\n            noise_pred = out.sample\n        if eta_is_zero:\n            # 2. compute more noisy image and set x_t -> x_t+1\n            xt = forward_step(model, noise_pred, t, xt)\n\n        else: \n            # xtm1 =  xts[idx+1][None]\n            xtm1 =  xts[idx][None]\n            # pred of x0\n            pred_original_sample = (xt - (1-alpha_bar[t])  ** 0.5 * noise_pred ) / alpha_bar[t] ** 0.5\n            \n            # direction to xt\n            prev_timestep = t - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n            alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n            \n            variance = get_variance(model, t)\n            pred_sample_direction = (1 - alpha_prod_t_prev - etas[idx] * variance ) ** (0.5) * noise_pred\n\n            mu_xt = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n\n            z = (xtm1 - mu_xt ) / ( etas[idx] * variance ** 0.5 )\n            zs[idx] = z\n\n            # correction to avoid error accumulation\n            xtm1 = mu_xt + ( etas[idx] * variance ** 0.5 )*z\n            xts[idx] = xtm1\n\n    if not zs is None: \n        zs[0] = torch.zeros_like(zs[0]) \n\n    return xt, zs, xts\n\n\ndef reverse_step(model, model_output, timestep, sample, eta = 0, variance_noise=None):\n    # 1. get previous step value (=t-1)\n    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n    # 2. compute alphas, betas\n    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n    beta_prod_t = 1 - alpha_prod_t\n    # 3. compute predicted original sample from predicted noise also called\n    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n    # 5. compute variance: \"sigma_t()\" -> see formula (16)\n    # _t = sqrt((1  _t1)/(1  _t)) * sqrt(1  _t/_t1)    \n    # variance = self.scheduler._get_variance(timestep, prev_timestep)\n    variance = get_variance(model, timestep) #, prev_timestep)\n    std_dev_t = eta * variance ** (0.5)\n    # Take care of asymetric reverse process (asyrp)\n    model_output_direction = model_output\n    # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    # pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * model_output_direction\n    pred_sample_direction = (1 - alpha_prod_t_prev - eta * variance) ** (0.5) * model_output_direction\n    # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n    # 8. Add noice if eta > 0\n    if eta > 0:\n        if variance_noise is None:\n            variance_noise = torch.randn(model_output.shape, device=model.device)\n        sigma_z =  eta * variance ** (0.5) * variance_noise\n        prev_sample = prev_sample + sigma_z\n\n    return prev_sample\n\ndef inversion_reverse_process(tokenizer, custom_model, model,\n                    xT, \n                    etas = 0,\n                    prompts = \"\",\n                    cfg_scales = None,\n                    prog_bar = False,\n                    zs = None,\n                    controller=None,\n                    asyrp = False):\n\n    batch_size = len(prompts)\n\n    cfg_scales_tensor = torch.Tensor(cfg_scales).view(-1,1,1,1).to(model.device)\n\n    text_embeddings = encode_text(model, prompts, tokenizer, custom_model)\n    uncond_embedding = encode_text(model, [\"\"] * batch_size, tokenizer, custom_model)\n\n    if etas is None: etas = 0\n    if type(etas) in [int, float]: etas = [etas]*model.scheduler.num_inference_steps\n    assert len(etas) == model.scheduler.num_inference_steps\n    timesteps = model.scheduler.timesteps.to(model.device)\n\n    xt = xT.expand(batch_size, -1, -1, -1)\n    op = timesteps[-zs.shape[0]:] \n\n    t_to_idx = {int(v):k for k,v in enumerate(timesteps[-zs.shape[0]:])}\n\n    for t in op:\n        idx = model.scheduler.num_inference_steps-t_to_idx[int(t)]-(model.scheduler.num_inference_steps-zs.shape[0]+1)    \n        ## Unconditional embedding\n        with torch.no_grad():\n            uncond_out = model.unet.forward(xt, timestep =  t, \n                                            encoder_hidden_states = uncond_embedding)\n\n            ## Conditional embedding  \n        if prompts:  \n            with torch.no_grad():\n                cond_out = model.unet.forward(xt, timestep =  t, \n                                                encoder_hidden_states = text_embeddings)\n            \n        \n        z = zs[idx] if not zs is None else None\n        z = z.expand(batch_size, -1, -1, -1)\n        if prompts:\n            ## classifier free guidance\n            noise_pred = uncond_out.sample + cfg_scales_tensor * (cond_out.sample - uncond_out.sample)\n        else: \n            noise_pred = uncond_out.sample\n        # 2. compute less noisy image and set x_t -> x_t-1  \n        xt = reverse_step(model, noise_pred, t, xt, eta = etas[idx], variance_noise = z) \n        if controller is not None:\n            xt = controller.step_callback(xt)        \n    return xt, zs\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:19:40.234844Z","iopub.execute_input":"2024-11-29T19:19:40.235395Z","iopub.status.idle":"2024-11-29T19:19:40.247828Z","shell.execute_reply.started":"2024-11-29T19:19:40.235361Z","shell.execute_reply":"2024-11-29T19:19:40.246950Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/PnPInversion/models/edit_friendly_ddm/inversion_utils.py\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"%%writefile \"/kaggle/working/PnPInversion/models/edit_friendly_ddm/inversion_utils.py\"\n# %load \"/kaggle/working/PnPInversion/models/edit_friendly_ddm/inversion_utils.py\"\nimport torch\nimport os\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, AutoModel\nclass CustomLlamaModel(torch.nn.Module):\n    def __init__(self):\n        super(CustomLlamaModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n        self.projection = torch.nn.Linear(self.base_model.config.hidden_size, 1024)\n\n    def forward(self, **kwargs):\n        # Pass inputs through the base model\n        outputs = self.base_model(**kwargs)\n        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n        \n        # Project the embeddings to the desired dimension\n        projected_embeddings = self.projection(last_hidden_state)  # Shape: (batch_size, seq_len, target_dim)\n        return projected_embeddings\n        \ndef load_real_image(folder = \"data/\", img_name = None, idx = 0, img_size=512, device='cuda'):\n    from ddm_inversion.utils import pil_to_tensor\n    from PIL import Image\n    from glob import glob\n    if img_name is not None:\n        path = os.path.join(folder, img_name)\n    else:\n        path = glob(folder + \"*\")[idx]\n\n    img = Image.open(path).resize((img_size,\n                                    img_size))\n\n    img = pil_to_tensor(img).to(device)\n\n    if img.shape[1]== 4:\n        img = img[:,:3,:,:]\n    return img\n\ndef mu_tilde(model, xt,x0, timestep):\n    \"mu_tilde(x_t, x_0) DDPM paper eq. 7\"\n    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n    alpha_t = model.scheduler.alphas[timestep]\n    beta_t = 1 - alpha_t \n    alpha_bar = model.scheduler.alphas_cumprod[timestep]\n    return ((alpha_prod_t_prev ** 0.5 * beta_t) / (1-alpha_bar)) * x0 +  ((alpha_t**0.5 *(1-alpha_prod_t_prev)) / (1- alpha_bar))*xt\n\ndef sample_xts_from_x0(model, x0, num_inference_steps=50):\n    \"\"\"\n    Samples from P(x_1:T|x_0)\n    \"\"\"\n    # torch.manual_seed(43256465436)\n    alpha_bar = model.scheduler.alphas_cumprod\n    sqrt_one_minus_alpha_bar = (1-alpha_bar) ** 0.5\n    alphas = model.scheduler.alphas\n    betas = 1 - alphas\n    variance_noise_shape = (\n            num_inference_steps,\n            model.unet.in_channels, \n            model.unet.sample_size,\n            model.unet.sample_size)\n    \n    timesteps = model.scheduler.timesteps.to(model.device)\n    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n    xts = torch.zeros((num_inference_steps+1,model.unet.in_channels, model.unet.sample_size, model.unet.sample_size)).to(x0.device)\n    xts[0] = x0\n    for t in reversed(timesteps):\n        idx = num_inference_steps-t_to_idx[int(t)]\n        xts[idx] = x0 * (alpha_bar[t] ** 0.5) +  torch.randn_like(x0) * sqrt_one_minus_alpha_bar[t]\n\n\n    return xts\n\n\ndef encode_text(model, prompts):\n    print(f\"prompts: {prompts}\")\n    target_dim = 768\n    model_name = \"openlm-research/open_llama_3b_v2\"\n\n    custom_model = CustomLlamaModel()\n\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n\n\n    text_input = tokenizer(\n        prompts,\n        padding=\"max_length\",\n        max_length=77, \n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    print(text_input)\n    if \"token_type_ids\" in text_input:\n        del text_input[\"token_type_ids\"]\n        \n    with torch.no_grad():\n        text_encoding = custom_model(**text_input).to(model.device)\n        print(f\"text_encoding: {text_encoding.shape}\")\n        \n    return text_encoding\n\ndef forward_step(model, model_output, timestep, sample):\n    next_timestep = min(model.scheduler.config.num_train_timesteps - 2,\n                        timestep + model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps)\n\n    # 2. compute alphas, betas\n    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n    # alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep] if next_ltimestep >= 0 else self.scheduler.final_alpha_cumprod\n\n    beta_prod_t = 1 - alpha_prod_t\n\n    # 3. compute predicted original sample from predicted noise also called\n    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n\n    # 5. TODO: simple noising implementatiom\n    next_sample = model.scheduler.add_noise(pred_original_sample,\n                                    model_output,\n                                    torch.LongTensor([next_timestep]))\n    return next_sample\n\n\ndef get_variance(model, timestep): #, prev_timestep):\n    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n    return variance\n\ndef inversion_forward_process(model, x0, \n                            etas = None,    \n                            prog_bar = False,\n                            prompt = \"\",\n                            cfg_scale = 3.5,\n                            num_inference_steps=50, eps = None):\n\n    if not prompt==\"\":\n        text_embeddings = encode_text(model, prompt)\n    uncond_embedding = encode_text(model, \"\")\n    timesteps = model.scheduler.timesteps.to(model.device)\n    variance_noise_shape = (\n        num_inference_steps,\n        model.unet.in_channels, \n        model.unet.sample_size,\n        model.unet.sample_size)\n    if etas is None or (type(etas) in [int, float] and etas == 0):\n        eta_is_zero = True\n        zs = None\n    else:\n        eta_is_zero = False\n        if type(etas) in [int, float]: etas = [etas]*model.scheduler.num_inference_steps\n        xts = sample_xts_from_x0(model, x0, num_inference_steps=num_inference_steps)\n        alpha_bar = model.scheduler.alphas_cumprod\n        zs = torch.zeros(size=variance_noise_shape, device=model.device)\n    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n    xt = x0\n    op = timesteps\n\n    for t in op:\n        # idx = t_to_idx[int(t)]\n        idx = num_inference_steps-t_to_idx[int(t)]-1\n        # 1. predict noise residual\n        if not eta_is_zero:\n            xt = xts[idx+1][None]\n            # xt = xts_cycle[idx+1][None]\n                    \n        with torch.no_grad():\n            out = model.unet.forward(xt, timestep =  t, encoder_hidden_states = uncond_embedding)\n            if not prompt==\"\":\n                cond_out = model.unet.forward(xt, timestep=t, encoder_hidden_states = text_embeddings)\n\n        if not prompt==\"\":\n            ## classifier free guidance\n            noise_pred = out.sample + cfg_scale * (cond_out.sample - out.sample)\n        else:\n            noise_pred = out.sample\n        if eta_is_zero:\n            # 2. compute more noisy image and set x_t -> x_t+1\n            xt = forward_step(model, noise_pred, t, xt)\n\n        else: \n            # xtm1 =  xts[idx+1][None]\n            xtm1 =  xts[idx][None]\n            # pred of x0\n            pred_original_sample = (xt - (1-alpha_bar[t])  ** 0.5 * noise_pred ) / alpha_bar[t] ** 0.5\n            \n            # direction to xt\n            prev_timestep = t - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n            alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n            \n            variance = get_variance(model, t)\n            pred_sample_direction = (1 - alpha_prod_t_prev - etas[idx] * variance ) ** (0.5) * noise_pred\n\n            mu_xt = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n\n            z = (xtm1 - mu_xt ) / ( etas[idx] * variance ** 0.5 )\n            zs[idx] = z\n\n            # correction to avoid error accumulation\n            xtm1 = mu_xt + ( etas[idx] * variance ** 0.5 )*z\n            xts[idx] = xtm1\n\n    if not zs is None: \n        zs[0] = torch.zeros_like(zs[0]) \n\n    return xt, zs, xts\n\n\ndef reverse_step(model, model_output, timestep, sample, eta = 0, variance_noise=None):\n    # 1. get previous step value (=t-1)\n    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n    # 2. compute alphas, betas\n    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n    beta_prod_t = 1 - alpha_prod_t\n    # 3. compute predicted original sample from predicted noise also called\n    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n    # 5. compute variance: \"sigma_t()\" -> see formula (16)\n    # _t = sqrt((1  _t1)/(1  _t)) * sqrt(1  _t/_t1)    \n    # variance = self.scheduler._get_variance(timestep, prev_timestep)\n    variance = get_variance(model, timestep) #, prev_timestep)\n    std_dev_t = eta * variance ** (0.5)\n    # Take care of asymetric reverse process (asyrp)\n    model_output_direction = model_output\n    # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    # pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * model_output_direction\n    pred_sample_direction = (1 - alpha_prod_t_prev - eta * variance) ** (0.5) * model_output_direction\n    # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n    # 8. Add noice if eta > 0\n    if eta > 0:\n        if variance_noise is None:\n            variance_noise = torch.randn(model_output.shape, device=model.device)\n        sigma_z =  eta * variance ** (0.5) * variance_noise\n        prev_sample = prev_sample + sigma_z\n\n    return prev_sample\n\ndef inversion_reverse_process(model,\n                    xT, \n                    etas = 0,\n                    prompts = \"\",\n                    cfg_scales = None,\n                    prog_bar = False,\n                    zs = None,\n                    controller=None,\n                    asyrp = False):\n\n    batch_size = len(prompts)\n\n    cfg_scales_tensor = torch.Tensor(cfg_scales).view(-1,1,1,1).to(model.device)\n\n    text_embeddings = encode_text(model, prompts)\n    uncond_embedding = encode_text(model, [\"\"] * batch_size)\n\n    if etas is None: etas = 0\n    if type(etas) in [int, float]: etas = [etas]*model.scheduler.num_inference_steps\n    assert len(etas) == model.scheduler.num_inference_steps\n    timesteps = model.scheduler.timesteps.to(model.device)\n\n    xt = xT.expand(batch_size, -1, -1, -1)\n    op = timesteps[-zs.shape[0]:] \n\n    t_to_idx = {int(v):k for k,v in enumerate(timesteps[-zs.shape[0]:])}\n\n    for t in op:\n        idx = model.scheduler.num_inference_steps-t_to_idx[int(t)]-(model.scheduler.num_inference_steps-zs.shape[0]+1)    \n        ## Unconditional embedding\n        with torch.no_grad():\n            uncond_out = model.unet.forward(xt, timestep =  t, \n                                            encoder_hidden_states = uncond_embedding)\n\n            ## Conditional embedding  \n        if prompts:  \n            with torch.no_grad():\n                cond_out = model.unet.forward(xt, timestep =  t, \n                                                encoder_hidden_states = text_embeddings)\n            \n        \n        z = zs[idx] if not zs is None else None\n        z = z.expand(batch_size, -1, -1, -1)\n        if prompts:\n            ## classifier free guidance\n            noise_pred = uncond_out.sample + cfg_scales_tensor * (cond_out.sample - uncond_out.sample)\n        else: \n            noise_pred = uncond_out.sample\n        # 2. compute less noisy image and set x_t -> x_t-1  \n        xt = reverse_step(model, noise_pred, t, xt, eta = etas[idx], variance_noise = z) \n        if controller is not None:\n            xt = controller.step_callback(xt)        \n    return xt, zs\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:54:06.886680Z","iopub.execute_input":"2024-11-29T08:54:06.887058Z","iopub.status.idle":"2024-11-29T08:54:06.899904Z","shell.execute_reply.started":"2024-11-29T08:54:06.887022Z","shell.execute_reply":"2024-11-29T08:54:06.899010Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/PnPInversion/models/edit_friendly_ddm/inversion_utils.py\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"%%writefile \"/kaggle/working/PnPInversion/run_editing_edit_friendly_p2p.py\"\n# %load \"/kaggle/working/PnPInversion/run_editing_edit_friendly_p2p.py\"\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers import DDIMScheduler\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom models.p2p.scheduler_dev import DDIMSchedulerDev\nimport json\nimport random\nimport argparse\nfrom torch import autocast, inference_mode\nfrom torch.nn import Linear, Module\nfrom utils.utils import load_512,txt_draw\nfrom models.edit_friendly_ddm.inversion_utils import inversion_forward_process, inversion_reverse_process\nfrom models.edit_friendly_ddm.ptp_classes import AttentionReplace,AttentionRefine,AttentionStore\nfrom models.edit_friendly_ddm.ptp_utils import register_attention_control\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n\ndef mask_decode(encoded_mask,image_shape=[512,512]):\n    length=image_shape[0]*image_shape[1]\n    mask_array=np.zeros((length,))\n    \n    for i in range(0,len(encoded_mask),2):\n        splice_len=min(encoded_mask[i+1],length-encoded_mask[i])\n        for j in range(splice_len):\n            mask_array[encoded_mask[i]+j]=1\n            \n    mask_array=mask_array.reshape(image_shape[0], image_shape[1])\n    # to avoid annotation errors in boundary\n    mask_array[0,:]=1\n    mask_array[-1,:]=1\n    mask_array[:,0]=1\n    mask_array[:,-1]=1\n            \n    return mask_array\n\n\ndef setup_seed(seed=1234):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nimage_save_paths={\n    \"edit-friendly-inversion+p2p\":\"edit-friendly-inversion+p2p\",\n    }\n\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device(\n    'cpu')\nNUM_DDIM_STEPS = 50\n#model_id=\"CompVis/stable-diffusion-v1-4\"\n#ldm_stable = StableDiffusionPipeline.from_pretrained(\n#    model_id).to(device)\n#ldm_stable.scheduler = DDIMScheduler.from_config(model_id, subfolder = \"scheduler\")\n#ldm_stable.scheduler.set_timesteps(NUM_DDIM_STEPS)\n\nprint(\"Starting Stable Diffusion...\")\nmodel_id = \"stabilityai/stable-diffusion-2-1-base\"\nldm_stable = StableDiffusionPipeline.from_pretrained(model_id).to(device)\nldm_stable.scheduler = DDIMScheduler.from_config(model_id, subfolder=\"scheduler\")\nldm_stable.scheduler.set_timesteps(NUM_DDIM_STEPS)\n\nETA=1\nSKIP=12\n\n\ndef edit_image_EF(edit_method,\n                  image_path,\n                    prompt_src,\n                    prompt_tar,\n                    source_guidance_scale=1,\n                    target_guidance_scale=7.5,cross_replace_steps=0.4,\n                    self_replace_steps=0.6\n                    ):\n    if edit_method==\"edit-friendly-inversion+p2p\":\n        image_gt = load_512(image_path)\n        \n        image_gt = torch.from_numpy(image_gt).float() / 127.5 - 1\n        image_gt = image_gt.permute(2, 0, 1).unsqueeze(0).to(device)\n        with autocast(\"cuda\"), inference_mode():\n            w0 = (ldm_stable.vae.encode(image_gt).latent_dist.mode() * 0.18215).float()\n            \n        controller = AttentionStore()\n        register_attention_control(ldm_stable, controller)\n            \n        wt, zs, wts = inversion_forward_process(ldm_stable, w0, etas=ETA, prompt=prompt_src, cfg_scale=source_guidance_scale, prog_bar=True, num_inference_steps=NUM_DDIM_STEPS)\n        \n        controller = AttentionStore()\n        register_attention_control(ldm_stable, controller)\n        \n        x0_reconstruct, _ = inversion_reverse_process(ldm_stable, xT=wts[NUM_DDIM_STEPS-SKIP], etas=ETA, prompts=[prompt_tar], cfg_scales=[target_guidance_scale], prog_bar=True, zs=zs[:(NUM_DDIM_STEPS-SKIP)], controller=controller)\n\n        cfg_scale_list = [source_guidance_scale, target_guidance_scale]\n        prompts = [prompt_src, prompt_tar]\n        if (len(prompt_src.split(\" \")) == len(prompt_tar.split(\" \"))):\n            controller = AttentionReplace(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, model=ldm_stable)\n        else:\n            # Should use Refine for target prompts with different number of tokens\n            controller = AttentionRefine(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, model=ldm_stable)\n\n        register_attention_control(ldm_stable, controller)\n        w0, _ = inversion_reverse_process(ldm_stable, xT=wts[NUM_DDIM_STEPS-SKIP], etas=ETA, prompts=prompts, cfg_scales=cfg_scale_list, prog_bar=True, zs=zs[:(NUM_DDIM_STEPS-SKIP)], controller=controller)\n        with autocast(\"cuda\"), inference_mode():\n            x0_dec = ldm_stable.vae.decode(1 / 0.18215 * w0[1].unsqueeze(0)).sample\n            x0_reconstruct_edit = ldm_stable.vae.decode(1 / 0.18215 * w0[0].unsqueeze(0)).sample\n            x0_reconstruct = ldm_stable.vae.decode(1 / 0.18215 * x0_reconstruct[0].unsqueeze(0)).sample\n            \n        image_instruct = txt_draw(f\"source prompt: {prompt_src}\\ntarget prompt: {prompt_tar}\")\n            \n        return Image.fromarray(np.concatenate(\n                                            (\n                                                image_instruct,\n                                                np.uint8((np.array(image_gt[0].permute(1,2,0).cpu().detach())/2+ 0.5)*255),\n                                                np.uint8((np.array(x0_reconstruct_edit[0].permute(1,2,0).cpu().detach())/2+ 0.5)*255),\n                                                np.uint8((np.array(x0_dec[0].permute(1,2,0).cpu().detach())/2+ 0.5)*255)\n                                            ),\n                                            1\n                                            )\n                            )\n    else:\n        raise NotImplementedError(f\"No edit method named {edit_method}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rerun_exist_images', action= \"store_true\") # rerun existing images\n    parser.add_argument('--data_path', type=str, default=\"data\") # the editing category that needed to run\n    parser.add_argument('--output_path', type=str, default=\"output\") # the editing category that needed to run\n    parser.add_argument('--edit_category_list', nargs = '+', type=str, default=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]) # the editing category that needed to run\n    parser.add_argument('--edit_method_list', nargs = '+', type=str, default=[\"edit-friendly-inversion+p2p\"]) # the editing methods that needed to run\n    args = parser.parse_args()\n    \n    rerun_exist_images=args.rerun_exist_images\n    data_path=args.data_path\n    output_path=args.output_path\n    edit_category_list=args.edit_category_list\n    edit_method_list=args.edit_method_list\n    \n    with open(f\"{data_path}/mapping_file.json\", \"r\") as f:\n        editing_instruction = json.load(f)\n    \n    for key, item in editing_instruction.items():\n        \n        if item[\"editing_type_id\"] not in edit_category_list:\n            continue\n        \n        original_prompt = item[\"original_prompt\"].replace(\"[\", \"\").replace(\"]\", \"\")\n        editing_prompt = item[\"editing_prompt\"].replace(\"[\", \"\").replace(\"]\", \"\")\n        image_path = os.path.join(f\"{data_path}/annotation_images\", item[\"image_path\"])\n        editing_instruction = item[\"editing_instruction\"]\n        blended_word = item[\"blended_word\"].split(\" \") if item[\"blended_word\"] != \"\" else []\n        mask = Image.fromarray(np.uint8(mask_decode(item[\"mask\"])[:,:,np.newaxis].repeat(3,2))).convert(\"L\")\n\n        for edit_method in edit_method_list:\n            present_image_save_path=image_path.replace(data_path, os.path.join(output_path,image_save_paths[edit_method]))\n            if ((not os.path.exists(present_image_save_path)) or rerun_exist_images):\n                print(f\"editing image [{image_path}] with [{edit_method}]\")\n                setup_seed()\n                torch.cuda.empty_cache()\n                edited_image = edit_image_EF(\n                        edit_method=edit_method,\n                        image_path=image_path,\n                        prompt_src=original_prompt,\n                        prompt_tar=editing_prompt,\n                        source_guidance_scale=1,\n                        target_guidance_scale=7.5,\n                        cross_replace_steps=0.4,\n                        self_replace_steps=0.6\n                        )\n                        \n                if not os.path.exists(os.path.dirname(present_image_save_path)):\n                    os.makedirs(os.path.dirname(present_image_save_path))\n                edited_image.save(present_image_save_path)\n                \n                print(f\"finish\")\n                \n            else:\n                print(f\"skip image [{image_path}] with [{edit_method}]\")\n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:12:22.310694Z","iopub.execute_input":"2024-11-29T08:12:22.311045Z","iopub.status.idle":"2024-11-29T08:12:22.321014Z","shell.execute_reply.started":"2024-11-29T08:12:22.311016Z","shell.execute_reply":"2024-11-29T08:12:22.320092Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/PnPInversion/run_editing_edit_friendly_p2p.py\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"!rm -rf /kaggle/working/PnPInversion/output/edit-friendly-inversion+p2p/annotation_images/0_random_140/000000000000.jpg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:05:44.891528Z","iopub.execute_input":"2024-11-29T08:05:44.892457Z","iopub.status.idle":"2024-11-29T08:05:45.897996Z","shell.execute_reply.started":"2024-11-29T08:05:44.892415Z","shell.execute_reply":"2024-11-29T08:05:45.896779Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"!source activate env && python -u /kaggle/working/PnPInversion/run_editing_edit_friendly_p2p.py \\\n    --data_path /kaggle/input/complete-pie-bench-dataset-v3 \\\n    --output_path /kaggle/working/PnPInversion/output \\\n    --edit_category_list 0 \\\n    --edit_method_list \"edit-friendly-inversion+p2p\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T00:36:46.376455Z","iopub.execute_input":"2024-11-30T00:36:46.377143Z","iopub.status.idle":"2024-11-30T00:43:51.490682Z","shell.execute_reply.started":"2024-11-30T00:36:46.377107Z","shell.execute_reply":"2024-11-30T00:43:51.489723Z"}},"outputs":[{"name":"stdout","text":"Starting Stable Diffusion...\ntext_encoder/pytorch_model.fp16.safetensors not found\nFetching 30 files: 100%|| 30/30 [00:00<00:00, 23185.76it/s]\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\nThe config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n/kaggle/working/miniconda/envs/env/lib/python3.9/site-packages/diffusers/configuration_utils.py:195: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\nSome weights of the model checkpoint at openlm-research/open_llama_3b_v2 were not used when initializing LlamaModel: ['lm_head.weight']\n- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nUsing pad_token, but it is not set yet.\nediting image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000000.jpg] with [edit-friendly-inversion+p2p]\nfinish\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000001.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000002.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000003.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000004.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000005.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000006.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000007.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000008.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000009.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000010.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000011.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000012.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000013.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000014.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000015.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000016.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000017.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000018.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000019.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000020.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000021.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000022.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000023.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000024.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000025.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000026.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000027.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000028.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000029.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000030.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000031.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000032.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000033.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000034.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000035.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000036.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000037.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000038.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000039.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000040.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000041.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000042.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000043.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000044.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000045.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000046.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000047.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000048.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000049.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000050.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000051.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000052.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000053.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000054.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000055.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000056.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000057.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000058.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000059.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000060.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000061.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000062.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000063.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000064.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000065.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000066.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000067.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000068.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000069.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000070.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000071.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000072.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000073.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000074.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000075.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000076.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000077.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000078.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000079.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000080.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000081.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000082.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000083.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000084.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000085.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000086.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000087.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000088.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000089.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000090.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000091.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000092.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000093.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000094.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000095.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000096.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000097.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000098.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000099.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000100.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000101.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000102.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000103.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000104.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000105.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000106.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000107.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000108.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000109.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000110.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000111.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000112.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000113.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000114.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000115.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000116.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000117.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000118.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000119.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000120.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000121.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000122.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000123.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000124.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000125.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000126.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000127.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000128.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000129.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000130.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000131.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000132.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000133.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000134.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000135.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000136.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000137.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000138.jpg] with [edit-friendly-inversion+p2p]\nskip image [/kaggle/input/complete-pie-bench-dataset-v3/annotation_images/0_random_140/000000000139.jpg] with [edit-friendly-inversion+p2p]\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"!source activate env && pip install scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T23:29:08.596722Z","iopub.execute_input":"2024-11-29T23:29:08.597525Z","iopub.status.idle":"2024-11-29T23:29:19.591633Z","shell.execute_reply.started":"2024-11-29T23:29:08.597482Z","shell.execute_reply":"2024-11-29T23:29:19.590766Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn\n  Downloading scikit_learn-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.19.5 in /kaggle/working/miniconda/envs/env/lib/python3.9/site-packages (from scikit-learn) (1.23.0)\nCollecting scipy>=1.6.0 (from scikit-learn)\n  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nCollecting joblib>=1.2.0 (from scikit-learn)\n  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting threadpoolctl>=3.1.0 (from scikit-learn)\n  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\nDownloading scikit_learn-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\nDownloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\nSuccessfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.13.1 threadpoolctl-3.5.0\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}